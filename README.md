# ğŸ“š Document QA Chatbot with RAG

A **Retrieval-Augmented Generation (RAG)** based chatbot that allows users to upload documents (PDF or TXT) and ask questions about their content. The system uses **semantic search** to retrieve relevant passages and generates accurate answers **based solely on the provided context**.

---

## ğŸš€ Features

- ğŸ“‚ **Document Upload**: Supports PDF and TXT files  
- âœ‚ï¸ **Intelligent Chunking**: Optimized for academic papers & speeches  
- ğŸ” **Semantic Search**: FAISS-based vector similarity search  
- ğŸ§  **Context-Aware Answers**: LLM responses grounded in document context  
- ğŸ’¬ **Interactive Chat Interface**: Built with Streamlit  
- ğŸ” **Context Transparency**: Shows exact passages used for answers  

---

## ğŸ“‹ Prerequisites

- Python **3.9+**  
- A **Groq API key** â†’ [Get it here](https://console.groq.com/)  

---

## ğŸ› ï¸ Installation

1. **Clone the repository**

    ```bash
    git clone https://github.com/harsh-thavai/rag_doc.git
    cd rag_doc
    ```

2. **Create a virtual environment**

    ```bash
    python -m venv venv
    source venv/bin/activate   # On Windows: venv\Scripts\activate
    ```

3. **Install dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Set up environment variables**
   - Create a `.env` file in the root directory.
   - Add your Groq API key:

        ```env
        GROQ_API_KEY=your_api_key_here
        ```

---

## ğŸƒ Running the Application

1. **Start the Streamlit app**

    ```bash
    streamlit run app.py
    ```

2. **Open your browser**
   - The app runs at: `http://localhost:8501`

3. **Using the app**
   - Upload a PDF or TXT document in the sidebar.  
   - Click **"Process Document"** to analyze.  
   - Ask questions in the chat interface.  
   - See the context passages behind every answer.  

---

---

## ğŸ§  How It Works

1. **Document Ingestion**  
   - PDF â†’ extracted via *PyPDF2*.  
   - TXT â†’ direct reading.

2. **Text Processing**  
   - Recursive character splitter with overlap.  
   - Custom separators optimized for research & speeches.

3. **Vectorization**  
   - Embeddings via `all-mpnet-base-v2` model.  
   - Captures semantic meaning.

4. **Vector Storage**  
   - FAISS index for similarity search.  
   - Enables fast nearest-neighbor lookups.

5. **Query Processing**  
   - User query â†’ embedding.  
   - Retrieve most relevant chunks.

6. **Answer Generation**  
   - Context + user query â†’ crafted prompt.  
   - **Llama 3** generates grounded answers through **Groq** API.  
   - If no answer is in the context â†’ the system acknowledges the limitation.

---

## ğŸ› ï¸ Technologies Used

- **Streamlit** â†’ UI framework  
- **LangChain** â†’ Text splitting utilities  
- **Sentence Transformers** â†’ Embeddings (`all-mpnet-base-v2`)  
- **FAISS** â†’ Fast vector similarity search  
- **PyPDF2** â†’ PDF text extraction  
- **Groq (Llama 3)** â†’ LLM for answer generation  
- **Python** â†’ Core language  

---

## ğŸ”§ Customization

- **Chunk Size & Overlap** â†’ `utils/text_splitter.py`  
- **Retrieval Parameters** â†’ `app4.py` (tune the number of chunks to retrieve)  
- **LLM Settings** â†’ `utils/llm_handler.py` (model, temperature, max tokens)  


